{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59570697",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Large XML ETL Workflow (Automated Flow)\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook demonstrates:\\n\",\n",
    "    \"- Splitting a large XML file into smaller files\\n\",\n",
    "    \"- Reading split XML files with Spark\\n\",\n",
    "    \"- Flattening and pivoting the data\\n\",\n",
    "    \"- Writing the result to a Delta table\\n\",\n",
    "    \"\\n\",\n",
    "    \"> **Adjust paths and parameters as needed for your environment!**\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Split Large XML File into Smaller Files\\n\",\n",
    "    \"\\n\",\n",
    "    \"This cell splits a large XML file into smaller files for parallel Spark processing.\\n\",\n",
    "    \"If you already have split files, you can skip this step.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Databricks Python cell\\n\",\n",
    "    \"import xml.etree.ElementTree as ET\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"\\n\",\n",
    "    \"input_file = '/dbfs/FileStore/large.xml'  # Path to your large XML file\\n\",\n",
    "    \"output_dir = '/dbfs/FileStore/orders_split'  # Output dir for split files\\n\",\n",
    "    \"chunk_size = 1000  # Orders per split file\\n\",\n",
    "    \"\\n\",\n",
    "    \"os.makedirs(output_dir, exist_ok=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"context = ET.iterparse(input_file, events=('end',))\\n\",\n",
    "    \"orders = []\\n\",\n",
    "    \"file_idx = 1\\n\",\n",
    "    \"\\n\",\n",
    "    \"for event, elem in context:\\n\",\n",
    "    \"    if elem.tag == 'order':\\n\",\n",
    "    \"        orders.append(ET.tostring(elem, encoding='unicode'))\\n\",\n",
    "    \"        if len(orders) == chunk_size:\\n\",\n",
    "    \"            with open(f'{output_dir}/orders_part_{file_idx}.xml', 'w', encoding='utf-8') as f:\\n\",\n",
    "    \"                f.write(f'<orders>\\\\n{\\\"\\\".join(orders)}\\\\n</orders>')\\n\",\n",
    "    \"            orders = []\\n\",\n",
    "    \"            file_idx += 1\\n\",\n",
    "    \"        elem.clear()\\n\",\n",
    "    \"\\n\",\n",
    "    \"if orders:\\n\",\n",
    "    \"    with open(f'{output_dir}/orders_part_{file_idx}.xml', 'w', encoding='utf-8') as f:\\n\",\n",
    "    \"        f.write(f'<orders>\\\\n{\\\"\\\".join(orders)}\\\\n</orders>')\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Splitting complete. {file_idx} files created in {output_dir}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Read XML Files with Spark\\n\",\n",
    "    \"\\n\",\n",
    "    \"This cell reads all split XML files into a Spark DataFrame.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from pyspark.sql import functions as F\\n\",\n",
    "    \"\\n\",\n",
    "    \"xml_path = \\\"dbfs:/FileStore/orders_split/*.xml\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"df = spark.read.format(\\\"xml\\\") \\\\\\n\",\n",
    "    \"    .option(\\\"rowTag\\\", \\\"order\\\") \\\\\\n\",\n",
    "    \"    .load(xml_path)\\n\",\n",
    "    \"\\n\",\n",
    "    \"df.printSchema()\\n\",\n",
    "    \"display(df.limit(5))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Flatten and Pivot the Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"This cell flattens the nested structure and pivots product/qty pairs.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from pyspark.sql.window import Window\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Flatten\\n\",\n",
    "    \"df_flat = df \\\\\\n\",\n",
    "    \"    .withColumn(\\\"order_id\\\", F.col(\\\"id\\\")) \\\\\\n\",\n",
    "    \"    .withColumn(\\\"customer_name\\\", F.col(\\\"customer.name\\\")) \\\\\\n\",\n",
    "    \"    .withColumn(\\\"customer_email\\\", F.col(\\\"customer.email\\\")) \\\\\\n\",\n",
    "    \"    .withColumn(\\\"notes\\\", F.col(\\\"notes\\\")) \\\\\\n\",\n",
    "    \"    .withColumn(\\\"item\\\", F.explode_outer(\\\"items.item\\\")) \\\\\\n\",\n",
    "    \"    .withColumn(\\\"product\\\", F.col(\\\"item.product\\\")) \\\\\\n\",\n",
    "    \"    .withColumn(\\\"qty\\\", F.col(\\\"item.qty\\\")) \\\\\\n\",\n",
    "    \"    .select(\\n\",\n",
    "    \"        \\\"order_id\\\", \\\"customer_name\\\", \\\"customer_email\\\", \\\"product\\\", \\\"qty\\\", \\\"notes\\\"\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add row number per item in each order\\n\",\n",
    "    \"windowSpec = Window.partitionBy(\\\"order_id\\\").orderBy(F.lit(1))\\n\",\n",
    "    \"df_numbered = df_flat.withColumn(\\\"prod_num\\\", F.row_number().over(windowSpec))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Pivot columns\\n\",\n",
    "    \"max_products = 10  # Adjust as needed\\n\",\n",
    "    \"cols = []\\n\",\n",
    "    \"for i in range(1, max_products + 1):\\n\",\n",
    "    \"    cols.append(F.max(F.when(F.col(\\\"prod_num\\\") == i, F.col(\\\"product\\\"))).alias(f\\\"Product{i}\\\"))\\n\",\n",
    "    \"    cols.append(F.max(F.when(F.col(\\\"prod_num\\\") == i, F.col(\\\"qty\\\"))).alias(f\\\"Qty{i}\\\"))\\n\",\n",
    "    \"\\n\",\n",
    "    \"df_pivot = df_numbered.groupBy(\\\"order_id\\\", \\\"customer_name\\\", \\\"customer_email\\\", \\\"notes\\\").agg(*cols)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Select columns in desired order\\n\",\n",
    "    \"select_cols = [\\\"order_id\\\", \\\"customer_name\\\", \\\"customer_email\\\"]\\n\",\n",
    "    \"for i in range(1, max_products + 1):\\n\",\n",
    "    \"    select_cols.append(f\\\"Product{i}\\\")\\n\",\n",
    "    \"    select_cols.append(f\\\"Qty{i}\\\")\\n\",\n",
    "    \"select_cols.append(\\\"notes\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"df_pivot = df_pivot.select(*select_cols)\\n\",\n",
    "    \"display(df_pivot.limit(5))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Write to Delta Table\\n\",\n",
    "    \"\\n\",\n",
    "    \"This cell writes the final DataFrame to a Delta table for analytics.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_pivot.write.format(\\\"delta\\\").mode(\\\"overwrite\\\").saveAsTable(\\\"orders_pivoted\\\")\\n\",\n",
    "    \"print(\\\"Data written to Delta table 'orders_pivoted'\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\",\n",
    "   \"version\": \"\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 2\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
